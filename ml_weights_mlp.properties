# MLP (Multi-Layer Perceptron) Neural Network Weights
# Model Type: MLP
# Architecture: 6 -> 8 -> 4 -> 1
# Activation: relu
# Trained on 990000 samples
# Test R² Score: 0.9659, RMSE: 27.2582
# Feature order: manhattan_dist, euclidean_dist, pct_maintenance, pct_hightraffic, pct_shortcut, pct_wall

MODEL_TYPE=MLP
INPUT_SIZE=6
HIDDEN1_SIZE=8
HIDDEN2_SIZE=4
OUTPUT_SIZE=1

# Layer 1 Weights (8x6): Hidden1 = relu(Input * W1 + b1)
L1_W_0_0=-0.024006
L1_W_1_0=0.049725
L1_W_2_0=-3.741822
L1_W_3_0=-0.184274
L1_W_4_0=6.337191
L1_W_5_0=-4.850136
L1_W_0_1=0.540079
L1_W_1_1=0.222390
L1_W_2_1=12.635143
L1_W_3_1=15.737878
L1_W_4_1=12.249977
L1_W_5_1=14.172862
L1_W_0_2=0.389751
L1_W_1_2=-0.541095
L1_W_2_2=-7.296386
L1_W_3_2=-0.037189
L1_W_4_2=10.215449
L1_W_5_2=-8.210044
L1_W_0_3=0.234796
L1_W_1_3=0.720875
L1_W_2_3=4.703611
L1_W_3_3=-2.027425
L1_W_4_3=-13.321500
L1_W_5_3=6.060627
L1_W_0_4=-0.241957
L1_W_1_4=0.643672
L1_W_2_4=5.540644
L1_W_3_4=-2.328341
L1_W_4_4=-15.705132
L1_W_5_4=5.610679
L1_W_0_5=0.019702
L1_W_1_5=0.079650
L1_W_2_5=-7.984411
L1_W_3_5=-1.582725
L1_W_4_5=7.508359
L1_W_5_5=-8.545881
L1_W_0_6=-0.001472
L1_W_1_6=0.031309
L1_W_2_6=-8.944125
L1_W_3_6=0.158249
L1_W_4_6=9.386740
L1_W_5_6=-8.655589
L1_W_0_7=0.597909
L1_W_1_7=-0.296040
L1_W_2_7=5.876890
L1_W_3_7=4.439271
L1_W_4_7=-0.040960
L1_W_5_7=7.247583

# Layer 1 Biases (8)
L1_B_0=3.498101
L1_B_1=3.248817
L1_B_2=3.625477
L1_B_3=-5.726212
L1_B_4=-5.500440
L1_B_5=3.055597
L1_B_6=1.457207
L1_B_7=-2.022027

# Layer 2 Weights (4x8): Hidden2 = relu(Hidden1 * W2 + b2)
L2_W_0_0=-1.481631
L2_W_1_0=-0.176255
L2_W_2_0=0.770037
L2_W_3_0=0.294649
L2_W_4_0=0.550740
L2_W_5_0=2.446846
L2_W_6_0=-2.254685
L2_W_7_0=0.039342
L2_W_0_1=-3.988645
L2_W_1_1=-0.178893
L2_W_2_1=-9.129528
L2_W_3_1=1.002652
L2_W_4_1=1.485237
L2_W_5_1=-7.811015
L2_W_6_1=-4.467696
L2_W_7_1=0.029529
L2_W_0_2=-1.502786
L2_W_1_2=0.913739
L2_W_2_2=-7.255079
L2_W_3_2=0.803070
L2_W_4_2=0.746001
L2_W_5_2=-4.678224
L2_W_6_2=-0.158267
L2_W_7_2=0.855432
L2_W_0_3=-1.432278
L2_W_1_3=-0.264325
L2_W_2_3=0.150442
L2_W_3_3=0.171975
L2_W_4_3=0.628335
L2_W_5_3=2.258671
L2_W_6_3=1.887320
L2_W_7_3=-0.009099

# Layer 2 Biases (4)
L2_B_0=-6.147421
L2_B_1=-9.805039
L2_B_2=-2.486410
L2_B_3=-6.220374

# Layer 3 Weights (1x4): Output = Hidden2 * W3 + b3 (no activation)
L3_W_0_0=-1.686516

# Layer 3 Bias (1)
L3_B_0=9.322839
